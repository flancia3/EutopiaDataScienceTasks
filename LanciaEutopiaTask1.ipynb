{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Frederick Lancia\n",
    "Eutopia Task 1\n",
    "April 28"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  language                                              words\n",
      "0       en  [[privacy policy, privacy, cookie, cookies, po...\n",
      "contact\n",
      "privacy policy\n",
      "terms\n",
      "about\n",
      "contact link: https://www.powerup-tech.com//contact\n",
      "privacy link: https://www.powerup-tech.com//legal/privacy-policy\n",
      "about link: https://www.powerup-tech.com//about\n",
      "terms and conditions link: https://www.powerup-tech.com//legal/terms-of-service\n",
      "  language                                              words\n",
      "0       en  [[privacy policy, privacy, cookie, cookies, po...\n",
      "1       ru  [[privacy policy, privacy, cookie, cookies, po...\n",
      "2       it  [[privacy policy, privacy, cookie, cookies, po...\n",
      "3       fi  [[privacy policy, privacy, cookie, cookies, po...\n",
      "4       nl  [[privacy policy, privacy, cookie, cookies, po...\n",
      "5       de  [[privacy policy, privacy, cookie, cookies, po...\n",
      "6       fr  [[privacy policy, privacy, cookie, cookies, po...\n",
      "7       es  [[privacy policy, privacy, cookie, cookies, po...\n",
      "8       et  [[privacy policy, privacy, cookie, cookies, po...\n",
      "9       xh  [[privacy policy, privacy, cookie, cookies, po...\n",
      "  language                                              words\n",
      "0       en  [[privacy policy, privacy, cookie, cookies, po...\n",
      "legal\n",
      "conditions\n",
      "contact link: N/A\n",
      "privacy link: https://murfy.fr//mentions-legales-cgu\n",
      "about link: N/A\n",
      "terms and conditions link: https://murfy.fr//rendez-vous-reparateur/conditions-generales\n",
      "  language                                              words\n",
      "0       en  [[privacy policy, privacy, cookie, cookies, po...\n",
      "1       ru  [[privacy policy, privacy, cookie, cookies, po...\n",
      "2       it  [[privacy policy, privacy, cookie, cookies, po...\n",
      "3       fi  [[privacy policy, privacy, cookie, cookies, po...\n",
      "4       nl  [[privacy policy, privacy, cookie, cookies, po...\n",
      "5       de  [[privacy policy, privacy, cookie, cookies, po...\n",
      "6       fr  [[privacy policy, privacy, cookie, cookies, po...\n",
      "7       es  [[privacy policy, privacy, cookie, cookies, po...\n",
      "8       et  [[privacy policy, privacy, cookie, cookies, po...\n",
      "9       xh  [[privacy policy, privacy, cookie, cookies, po...\n",
      "  language                                              words\n",
      "0       en  [[privacy policy, privacy, cookie, cookies, po...\n",
      "contact\n",
      "privacy policy\n",
      "terms\n",
      "about\n",
      "contact link: https://woodie-milano.com//pages/contact-us_2022\n",
      "privacy link: https://woodie-milano.com//pages/privacy-policy\n",
      "about link: https://woodie-milano.com//pages/about-us\n",
      "terms and conditions link: https://woodie-milano.com//policies/terms-of-service\n",
      "  language                                              words\n",
      "0       en  [[privacy policy, privacy, cookie, cookies, po...\n",
      "1       ru  [[privacy policy, privacy, cookie, cookies, po...\n",
      "2       it  [[privacy policy, privacy, cookie, cookies, po...\n",
      "3       fi  [[privacy policy, privacy, cookie, cookies, po...\n",
      "4       nl  [[privacy policy, privacy, cookie, cookies, po...\n",
      "5       de  [[privacy policy, privacy, cookie, cookies, po...\n",
      "6       fr  [[privacy policy, privacy, cookie, cookies, po...\n",
      "7       es  [[privacy policy, privacy, cookie, cookies, po...\n",
      "8       et  [[privacy policy, privacy, cookie, cookies, po...\n",
      "9       xh  [[privacy policy, privacy, cookie, cookies, po...\n",
      "  language                                              words\n",
      "0       en  [[privacy policy, privacy, cookie, cookies, po...\n",
      "impressum\n",
      "impressum\n",
      "contact link: https://markta.at//i/impressum\n",
      "privacy link: https://markta.at//i/impressum\n",
      "about link: N/A\n",
      "terms and conditions link: N/A\n",
      "  language                                              words\n",
      "0       en  [[privacy policy, privacy, cookie, cookies, po...\n",
      "1       ru  [[privacy policy, privacy, cookie, cookies, po...\n",
      "2       it  [[privacy policy, privacy, cookie, cookies, po...\n",
      "3       fi  [[privacy policy, privacy, cookie, cookies, po...\n",
      "4       nl  [[privacy policy, privacy, cookie, cookies, po...\n",
      "5       de  [[privacy policy, privacy, cookie, cookies, po...\n",
      "6       fr  [[privacy policy, privacy, cookie, cookies, po...\n",
      "7       es  [[privacy policy, privacy, cookie, cookies, po...\n",
      "8       et  [[privacy policy, privacy, cookie, cookies, po...\n",
      "9       xh  [[privacy policy, privacy, cookie, cookies, po...\n",
      "  language                                              words\n",
      "5       de  [[privacy policy, privacy, cookie, cookies, po...\n",
      "contact link: N/A\n",
      "privacy link: N/A\n",
      "about link: N/A\n",
      "terms and conditions link: N/A\n",
      "  language                                              words\n",
      "0       en  [[privacy policy, privacy, cookie, cookies, po...\n",
      "1       ru  [[privacy policy, privacy, cookie, cookies, po...\n",
      "2       it  [[privacy policy, privacy, cookie, cookies, po...\n",
      "3       fi  [[privacy policy, privacy, cookie, cookies, po...\n",
      "4       nl  [[privacy policy, privacy, cookie, cookies, po...\n",
      "5       de  [[privacy policy, privacy, cookie, cookies, po...\n",
      "6       fr  [[privacy policy, privacy, cookie, cookies, po...\n",
      "7       es  [[privacy policy, privacy, cookie, cookies, po...\n",
      "8       et  [[privacy policy, privacy, cookie, cookies, po...\n",
      "9       xh  [[privacy policy, privacy, cookie, cookies, po...\n",
      "  language                                              words\n",
      "0       en  [[privacy policy, privacy, cookie, cookies, po...\n",
      "impressum\n",
      "privacy\n",
      "legal\n",
      "contact link: https://www.kulero.de//pages/impressum\n",
      "privacy link: https://www.kulero.de//policies/privacy-policy\n",
      "about link: N/A\n",
      "terms and conditions link: https://www.kulero.de//policies/legal-notice\n",
      "  language                                              words\n",
      "0       en  [[privacy policy, privacy, cookie, cookies, po...\n",
      "1       ru  [[privacy policy, privacy, cookie, cookies, po...\n",
      "2       it  [[privacy policy, privacy, cookie, cookies, po...\n",
      "3       fi  [[privacy policy, privacy, cookie, cookies, po...\n",
      "4       nl  [[privacy policy, privacy, cookie, cookies, po...\n",
      "5       de  [[privacy policy, privacy, cookie, cookies, po...\n",
      "6       fr  [[privacy policy, privacy, cookie, cookies, po...\n",
      "7       es  [[privacy policy, privacy, cookie, cookies, po...\n",
      "8       et  [[privacy policy, privacy, cookie, cookies, po...\n",
      "9       xh  [[privacy policy, privacy, cookie, cookies, po...\n",
      "   language                                              words\n",
      "10       no  [[privacy policy, privacy, cookie, cookies, po...\n",
      "contact link: N/A\n",
      "privacy link: N/A\n",
      "about link: N/A\n",
      "terms and conditions link: N/A\n",
      "   language                                              words\n",
      "0        en  [[privacy policy, privacy, cookie, cookies, po...\n",
      "1        ru  [[privacy policy, privacy, cookie, cookies, po...\n",
      "2        it  [[privacy policy, privacy, cookie, cookies, po...\n",
      "3        fi  [[privacy policy, privacy, cookie, cookies, po...\n",
      "4        nl  [[privacy policy, privacy, cookie, cookies, po...\n",
      "5        de  [[privacy policy, privacy, cookie, cookies, po...\n",
      "6        fr  [[privacy policy, privacy, cookie, cookies, po...\n",
      "7        es  [[privacy policy, privacy, cookie, cookies, po...\n",
      "8        et  [[privacy policy, privacy, cookie, cookies, po...\n",
      "9        xh  [[privacy policy, privacy, cookie, cookies, po...\n",
      "10       no  [[privacy policy, privacy, cookie, cookies, po...\n",
      "  language                                              words\n",
      "0       en  [[privacy policy, privacy, cookie, cookies, po...\n",
      "privacy policy\n",
      "about\n",
      "contact link: N/A\n",
      "privacy link: https://winningfoods.nl/privacy-policy/\n",
      "about link: https://winningfoods.nl/about/\n",
      "terms and conditions link: N/A\n",
      "   language                                              words\n",
      "0        en  [[privacy policy, privacy, cookie, cookies, po...\n",
      "1        ru  [[privacy policy, privacy, cookie, cookies, po...\n",
      "2        it  [[privacy policy, privacy, cookie, cookies, po...\n",
      "3        fi  [[privacy policy, privacy, cookie, cookies, po...\n",
      "4        nl  [[privacy policy, privacy, cookie, cookies, po...\n",
      "5        de  [[privacy policy, privacy, cookie, cookies, po...\n",
      "6        fr  [[privacy policy, privacy, cookie, cookies, po...\n",
      "7        es  [[privacy policy, privacy, cookie, cookies, po...\n",
      "8        et  [[privacy policy, privacy, cookie, cookies, po...\n",
      "9        xh  [[privacy policy, privacy, cookie, cookies, po...\n",
      "10       no  [[privacy policy, privacy, cookie, cookies, po...\n",
      "  language                                              words\n",
      "0       en  [[privacy policy, privacy, cookie, cookies, po...\n",
      "contact\n",
      "imprint\n",
      "contact link: https://www.greencloudnine.com/info#section-contact\n",
      "privacy link: https://www.greencloudnine.com/info/imprint\n",
      "about link: N/A\n",
      "terms and conditions link: N/A\n",
      "   language                                              words\n",
      "0        en  [[privacy policy, privacy, cookie, cookies, po...\n",
      "1        ru  [[privacy policy, privacy, cookie, cookies, po...\n",
      "2        it  [[privacy policy, privacy, cookie, cookies, po...\n",
      "3        fi  [[privacy policy, privacy, cookie, cookies, po...\n",
      "4        nl  [[privacy policy, privacy, cookie, cookies, po...\n",
      "5        de  [[privacy policy, privacy, cookie, cookies, po...\n",
      "6        fr  [[privacy policy, privacy, cookie, cookies, po...\n",
      "7        es  [[privacy policy, privacy, cookie, cookies, po...\n",
      "8        et  [[privacy policy, privacy, cookie, cookies, po...\n",
      "9        xh  [[privacy policy, privacy, cookie, cookies, po...\n",
      "10       no  [[privacy policy, privacy, cookie, cookies, po...\n",
      "  language                                              words\n",
      "0       en  [[privacy policy, privacy, cookie, cookies, po...\n",
      "contact link: N/A\n",
      "privacy link: N/A\n",
      "about link: N/A\n",
      "terms and conditions link: N/A\n",
      "   language                                              words\n",
      "0        en  [[privacy policy, privacy, cookie, cookies, po...\n",
      "1        ru  [[privacy policy, privacy, cookie, cookies, po...\n",
      "2        it  [[privacy policy, privacy, cookie, cookies, po...\n",
      "3        fi  [[privacy policy, privacy, cookie, cookies, po...\n",
      "4        nl  [[privacy policy, privacy, cookie, cookies, po...\n",
      "5        de  [[privacy policy, privacy, cookie, cookies, po...\n",
      "6        fr  [[privacy policy, privacy, cookie, cookies, po...\n",
      "7        es  [[privacy policy, privacy, cookie, cookies, po...\n",
      "8        et  [[privacy policy, privacy, cookie, cookies, po...\n",
      "9        xh  [[privacy policy, privacy, cookie, cookies, po...\n",
      "10       no  [[privacy policy, privacy, cookie, cookies, po...\n",
      "  language                                              words\n",
      "0       en  [[privacy policy, privacy, cookie, cookies, po...\n",
      "contact link: N/A\n",
      "privacy link: N/A\n",
      "about link: N/A\n",
      "terms and conditions link: N/A\n",
      "   language                                              words\n",
      "0        en  [[privacy policy, privacy, cookie, cookies, po...\n",
      "1        ru  [[privacy policy, privacy, cookie, cookies, po...\n",
      "2        it  [[privacy policy, privacy, cookie, cookies, po...\n",
      "3        fi  [[privacy policy, privacy, cookie, cookies, po...\n",
      "4        nl  [[privacy policy, privacy, cookie, cookies, po...\n",
      "5        de  [[privacy policy, privacy, cookie, cookies, po...\n",
      "6        fr  [[privacy policy, privacy, cookie, cookies, po...\n",
      "7        es  [[privacy policy, privacy, cookie, cookies, po...\n",
      "8        et  [[privacy policy, privacy, cookie, cookies, po...\n",
      "9        xh  [[privacy policy, privacy, cookie, cookies, po...\n",
      "10       no  [[privacy policy, privacy, cookie, cookies, po...\n",
      "  language                                              words\n",
      "0       en  [[privacy policy, privacy, cookie, cookies, po...\n",
      "contact link: N/A\n",
      "privacy link: N/A\n",
      "about link: N/A\n",
      "terms and conditions link: N/A\n",
      "   language                                              words\n",
      "0        en  [[privacy policy, privacy, cookie, cookies, po...\n",
      "1        ru  [[privacy policy, privacy, cookie, cookies, po...\n",
      "2        it  [[privacy policy, privacy, cookie, cookies, po...\n",
      "3        fi  [[privacy policy, privacy, cookie, cookies, po...\n",
      "4        nl  [[privacy policy, privacy, cookie, cookies, po...\n",
      "5        de  [[privacy policy, privacy, cookie, cookies, po...\n",
      "6        fr  [[privacy policy, privacy, cookie, cookies, po...\n",
      "7        es  [[privacy policy, privacy, cookie, cookies, po...\n",
      "8        et  [[privacy policy, privacy, cookie, cookies, po...\n",
      "9        xh  [[privacy policy, privacy, cookie, cookies, po...\n",
      "10       no  [[privacy policy, privacy, cookie, cookies, po...\n",
      "  language                                              words\n",
      "0       en  [[privacy policy, privacy, cookie, cookies, po...\n",
      "contact\n",
      "privacy policy\n",
      "terms\n",
      "about\n",
      "contact link: https://www.solidwater.life//pages/contact\n",
      "privacy link: https://www.solidwater.life//pages/privacy-policy\n",
      "about link: https://www.solidwater.life//pages/about-us\n",
      "terms and conditions link: https://www.solidwater.life//pages/terms-of-service\n",
      "   language                                              words\n",
      "0        en  [[privacy policy, privacy, cookie, cookies, po...\n",
      "1        ru  [[privacy policy, privacy, cookie, cookies, po...\n",
      "2        it  [[privacy policy, privacy, cookie, cookies, po...\n",
      "3        fi  [[privacy policy, privacy, cookie, cookies, po...\n",
      "4        nl  [[privacy policy, privacy, cookie, cookies, po...\n",
      "5        de  [[privacy policy, privacy, cookie, cookies, po...\n",
      "6        fr  [[privacy policy, privacy, cookie, cookies, po...\n",
      "7        es  [[privacy policy, privacy, cookie, cookies, po...\n",
      "8        et  [[privacy policy, privacy, cookie, cookies, po...\n",
      "9        xh  [[privacy policy, privacy, cookie, cookies, po...\n",
      "10       no  [[privacy policy, privacy, cookie, cookies, po...\n",
      "  language                                              words\n",
      "0       en  [[privacy policy, privacy, cookie, cookies, po...\n",
      "contact\n",
      "impressum\n",
      "terms\n",
      "about\n",
      "contact link: https://www.globalagrimonitor.org/kontakt/\n",
      "privacy link: https://www.globalagrimonitor.org/impressum/\n",
      "about link: https://www.globalagrimonitor.org/ueber-uns/\n",
      "terms and conditions link: https://www.globalagrimonitor.org/datenschutz/\n",
      "   language                                              words\n",
      "0        en  [[privacy policy, privacy, cookie, cookies, po...\n",
      "1        ru  [[privacy policy, privacy, cookie, cookies, po...\n",
      "2        it  [[privacy policy, privacy, cookie, cookies, po...\n",
      "3        fi  [[privacy policy, privacy, cookie, cookies, po...\n",
      "4        nl  [[privacy policy, privacy, cookie, cookies, po...\n",
      "5        de  [[privacy policy, privacy, cookie, cookies, po...\n",
      "6        fr  [[privacy policy, privacy, cookie, cookies, po...\n",
      "7        es  [[privacy policy, privacy, cookie, cookies, po...\n",
      "8        et  [[privacy policy, privacy, cookie, cookies, po...\n",
      "9        xh  [[privacy policy, privacy, cookie, cookies, po...\n",
      "10       no  [[privacy policy, privacy, cookie, cookies, po...\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from bs4 import BeautifulSoup as bs\n",
    "from pip._vendor import requests\n",
    "import pandas as pd\n",
    "from googletrans import Translator\n",
    "\n",
    "# link_type is a class for finding a link for a certain type\n",
    "# such as for the contact page\n",
    "# it takes a list of key words that would indicate high likelyhood\n",
    "# of being a link for such page. \n",
    "# it looks for those words first on the text over links and then\n",
    "# in the url itself\n",
    "# it makes the assumption that the text over a link is a slightly\n",
    "# better indicator of the link's type than the url itself\n",
    "\n",
    "class link_type:\n",
    "    def check_all_words(self):\n",
    "        final_link = \"N/A\"\n",
    "        for word in self.words:\n",
    "            # for every key word, ask if it is in some text over a link\n",
    "            final_link = self.check(word, 0)\n",
    "            # if found, break and return the link\n",
    "            if (final_link != \"N/A\"):\n",
    "                break \n",
    "            # ask if it is in the link itself\n",
    "            final_link = self.check(word, 1)\n",
    "            # if found, break and return the link\n",
    "            if (final_link != \"N/A\"):\n",
    "                break\n",
    "        return final_link\n",
    "\n",
    "\n",
    "    def check(self, word, num):\n",
    "        final_link = \"N/A\"\n",
    "        # for every link, ask if the word can be found there\n",
    "        # if num is 0, look in texts over links\n",
    "        # if num is 1, look in links themselves\n",
    "        for item in links:\n",
    "            if word in item[num].lower():\n",
    "                final_link = str(item[1])\n",
    "                return final_link\n",
    "        return final_link\n",
    "\n",
    "    def __init__(self, words):\n",
    "        self.words = words\n",
    "\n",
    "# paragraphs_finder is a class that takes the link to a page and breaks\n",
    "# the text into a list of strings of paragraphs\n",
    "# some groups of text appear as paragraphs on the site, but are separated\n",
    "# by different elements on the html. This keeps those groups of text\n",
    "# together and returns them to the way they appear on the site\n",
    "\n",
    "class paragraphs_finder:\n",
    "    def check(self):\n",
    "        link = self.link\n",
    "        if link == \"N/A\":\n",
    "            return \"\"\n",
    "        # use html parser to find all elements which might contain text\n",
    "        request = requests.get(link)\n",
    "        content = request.content\n",
    "        soup = bs(content)\n",
    "\n",
    "        paragraphs = soup.findAll(['p', 'h1', 'h2', 'h3', 'h4', 'h5', 'h6'])\n",
    "\n",
    "        # keep texts which come in the same larger elements together\n",
    "        # because sometimes texts which appear together on the visual site, do not on the html\n",
    "        # create a list of each of these larger groups of text\n",
    "        paragraphs_text = []\n",
    "        for paragraph in paragraphs:\n",
    "            paragraph_text = paragraph.text\n",
    "            paragraphs_text.append(paragraph.text)\n",
    "        return paragraphs_text\n",
    "\n",
    "    def __init__(self, link):\n",
    "        self.link = link\n",
    "\n",
    "# translate_lists is a class that takes a list of strings and appends translated \n",
    "# versions of each string (in the language of the current site) and returns them \n",
    "# as a larger list\n",
    "\n",
    "class translate_list:\n",
    "    def trans(self):\n",
    "        words = self.list.copy()\n",
    "        for word in self.list:\n",
    "            translated_word = translator.translate(word, dest=language, src='en')\n",
    "            words.append(translated_word.text)\n",
    "        return words\n",
    "\n",
    "    def __init__(self, list):\n",
    "        self.list = list\n",
    "\n",
    "# initialize translator for translating texts\n",
    "translator = Translator()\n",
    "# read data from given excel file containing company names and links\n",
    "company_data = pd.read_excel(r'/Users/fredericklancia/Downloads/InputData.xlsx')\n",
    "\n",
    "# create a list for storing language information, so we don't have to translate\n",
    "# more than once for a single language\n",
    "languages = []\n",
    "languages_and_translations = []\n",
    "\n",
    "# initialize the final SQL-like dataframe to be returned at the end of the program\n",
    "new_df = company_data.copy()\n",
    "new_df[\"about_us_page\"] = \"N/A\"\n",
    "new_df[\"contact_page\"] = \"N/A\"\n",
    "new_df[\"privacy_policy\"] = \"N/A\"\n",
    "new_df[\"terms_and_conditions\"] = \"N/A\"\n",
    "new_df[\"english_description\"] = \"N/A\"\n",
    "new_df[\"phone_numbers\"] = \"N/A\"\n",
    "new_df[\"emails\"] = \"N/A\"\n",
    "\n",
    "# define some generic patterns now which will be used later and don't rely on variables\n",
    "phone_pattern = r'([\\+]?(\\d{10,12}))'\n",
    "remove_spaces_pattern = r'[\\r\\n\\- ]+'\n",
    "email_pattern = r'[^@ \\t\\r\\n]+@[^@ \\t\\r\\n]+\\.[^@ \\t\\r\\n]+'\n",
    "\n",
    "# set lists of words to be used for finding these desired data pieces\n",
    "privacy_words = ['privacy policy', 'privacy', 'cookie', 'cookies', 'policy', 'impressum', 'imprint', 'legal']\n",
    "terms_and_conditions_words = ['terms', 'terms and conditions', 'terms of service', 'conditions', 'legal']\n",
    "contact_words = ['contact', 'reach out', 'in touch', 'impressum', 'imprint', 'contact us']\n",
    "about_words1 = ['about', 'about us' 'mission', 'what we do', 'who we are']\n",
    "about_words2 = ['find out more', 'learn more', 'more information', 'read more']\n",
    "about_words3 = ['history', 'people', 'description', 'overview']\n",
    "description_words = [\"we\"]\n",
    "\n",
    "language_words_df = pd.read_pickle('language_words.pkl')\n",
    "\"\"\"\"\"\n",
    "uncomment this code and delete this text if you want to run the program without the \n",
    "language file. You will also need to comment out the above line of code\n",
    "\n",
    "english_list = [privacy_words, terms_and_conditions_words, contact_words, about_words1, about_words2, about_words3, description_words]\n",
    "starter_data = [['en', english_list]]\n",
    "language_words_df = pd.DataFrame(starter_data, columns = ['language', 'words'])\n",
    "\"\"\"\n",
    "\n",
    "# iterate over each row of the given document, recall that each row represents\n",
    "# a company and website\n",
    "for i, row in company_data.iterrows():\n",
    "\n",
    "    company_link = row['website'] # extract link to home page\n",
    "\n",
    "    # these links return SSLError: HTTPSConnectionPool, so skip this iteration\n",
    "    if (company_link == 'https://www.airtight.ai/' or \n",
    "            company_link == 'https://agrodronegroup.ru/' or \n",
    "            company_link == 'https://fotonow.ai/' or\n",
    "            company_link == 'https://www.finsu.co.uk/' or\n",
    "            company_link == 'https://www.eupravnik.eu/' or\n",
    "            company_link == 'https://www.m-flowers.com/' or\n",
    "            company_link == 'https://www.relade.eu/'): continue\n",
    "\n",
    "    company_name = row['name'] # extract company name\n",
    "\n",
    "    # use beautiful soup api to parse html of site\n",
    "    home_request = requests.get(company_link)\n",
    "    home_content = home_request.content\n",
    "    home_soup = bs(home_content)\n",
    "\n",
    "    # split visual text into paragraphs, because some bits of text which\n",
    "    # should read together are in different elements\n",
    "    # use first paragraph to detect language of website\n",
    "    # assume that language remains constant throughout\n",
    "    paragraphs = home_soup.findAll(['p', 'h1', 'h2', 'h3', 'h4', 'h5', 'h6'])\n",
    "\n",
    "    # nearly all pages will have some paragraphs, if not, assume English\n",
    "    if paragraphs_text: language = translator.translate(paragraphs_text[0]).src\n",
    "    else: language = 'en'\n",
    "\n",
    "    # this translator API is extremely slow, so instead of translating the\n",
    "    # entire site into English, I will translate the words that I use to \n",
    "    # identify the desired data to the language of the site.\n",
    "    # I will hold onto the English words, in case parts of the site still\n",
    "    # use English. For example, many foreign sites will still use English URLS.\n",
    "\n",
    "    if language not in set(language_words_df['language']):\n",
    "        # data for this language doesn't yet exist in the data frame\n",
    "\n",
    "        new_language_list = [translate_list(privacy_words).trans(),\n",
    "                            translate_list(terms_and_conditions_words).trans(),\n",
    "                            translate_list(contact_words).trans(),\n",
    "                            translate_list(about_words1).trans(),\n",
    "                            translate_list(about_words2).trans(),\n",
    "                            translate_list(about_words3).trans(),\n",
    "                            translate_list(description_words).trans()]\n",
    "        new_row_data = pd.DataFrame([[language, new_language_list]],\n",
    "                            columns=['language', 'words'])\n",
    "        language_words_df = pd.concat([language_words_df, new_row_data], ignore_index=True)\n",
    "    # isolate the row of the language this article is in into it's own dataframe\n",
    "    language_row = language_words_df.loc[language_words_df['language'] == language]\n",
    "    print(language_row)\n",
    "\n",
    "    # extract word lists from the work list dataframe\n",
    "    privacy_words = language_row.iat[0,1][0]\n",
    "    terms_and_conditions_words = language_row.iat[0,1][1]\n",
    "    contact_words = language_row.iat[0,1][2]\n",
    "    about_words1 = language_row.iat[0,1][3]\n",
    "    about_words2 = language_row.iat[0,1][4]\n",
    "    about_words3 = language_row.iat[0,1][5]\n",
    "    description_words = language_row.iat[0,1][6]\n",
    "\n",
    "\n",
    "    # find all links to the site on the home page\n",
    "    # all links should be found in 'a' or 'p' elements\n",
    "    links = []\n",
    "    phone_numbers = []\n",
    "    emails = []\n",
    "\n",
    "    for a in home_soup.find_all(['a', 'p'], href=True):\n",
    "        link = a['href']\n",
    "\n",
    "        text = a.text\n",
    "        \n",
    "        # if the link contains the company link, its definately\n",
    "        # what we want\n",
    "        if company_link in link:\n",
    "            links.append([text, link])\n",
    "        else:\n",
    "            # this could be an incomplete link to the site\n",
    "            # or it could be a phone number or email\n",
    "\n",
    "            # see if an email address can be found in the link or its text\n",
    "            new_emails = re.findall(email_pattern, text, re.IGNORECASE)\n",
    "            new_emails.extend(re.findall(phone_pattern, link, re.IGNORECASE))\n",
    "            # if so, save it to emails\n",
    "            if new_emails:\n",
    "                for email in new_emails:\n",
    "                    emails.append(email)\n",
    "\n",
    "            # see if phone number can be found in the link or its text\n",
    "            # first, remove spaces, dashes, returns, etc\n",
    "            text_without_spaces = re.sub(remove_spaces_pattern, '', text)\n",
    "            # its just numbers, so no need to ignore case this time\n",
    "            new_numbers = re.findall(phone_pattern, text_without_spaces)\n",
    "            new_numbers.extend(re.findall(phone_pattern, link))\n",
    "\n",
    "            #if so, save them to phone_numbers\n",
    "            if new_numbers:\n",
    "                for number in new_numbers:\n",
    "                    phone_numbers.append(number)\n",
    "\n",
    "            # if no numbers or emails were found, and it doesn't contain\n",
    "            # a period, indicating a .com of some sort, its probably an\n",
    "            # incomplete link to the site, so save it to links\n",
    "            if (not new_numbers) & (not new_emails) & ('.'  not in link):\n",
    "                link = company_link + link\n",
    "                links.append([text, link])\n",
    "\n",
    "    # use found links and set key words to find links to desired pages of site\n",
    "    contact_link = link_type(contact_words).check_all_words()\n",
    "    privacy_link = link_type(privacy_words).check_all_words()\n",
    "    terms_and_conditions_link = link_type(terms_and_conditions_words).check_all_words()\n",
    "\n",
    "    # about us is a more complex concept such that a site may have more than one\n",
    "    # page about the company, but we must find the best one. That is why we need\n",
    "    # to look at different key words in a specific order. It also has a longer\n",
    "    # list of words because there are many semantically similar phrases.\n",
    "    # So, words are tested in order of 3 groups\n",
    "    about_link = link_type(about_words1).check_all_words()\n",
    "    if about_link == \"N/A\":\n",
    "        about_link = link_type(about_words2).check_all_words()\n",
    "        if about_link == \"N/A\":\n",
    "            about_link = link_type(about_words3).check_all_words()\n",
    "\n",
    "\n",
    "    # find text of paragraphs on each of these pages\n",
    "\n",
    "    # break text in all relevant pages into paragraphs for parsing\n",
    "    # we will then use these links and their paragraphs to find desired data\n",
    "    company_link_plus = [company_link, paragraphs_finder(company_link).check()]\n",
    "    contact_link_plus = [contact_link, paragraphs_finder(contact_link).check()]\n",
    "    privacy_link_plus = [privacy_link, paragraphs_finder(privacy_link).check()]\n",
    "    about_link_plus = [about_link, paragraphs_finder(about_link).check()]\n",
    "\n",
    "    # create lists for finding each page link\n",
    "    # let them contain links and the paragraphs on those pages\n",
    "    # the links are ordered from most to least likely to find the desired information\n",
    "    # contact_info_links is for finding phone numbers and emails\n",
    "    # for example, its highly likely that they will be found on contact page,\n",
    "    # that's why contact links come first, and about page links second\n",
    "    contact_info_links = [contact_link_plus, about_link_plus, company_link_plus]\n",
    "    # for finding description phrase\n",
    "    description_links = [about_link_plus, company_link_plus]\n",
    "    # add company name now to helpful words for finding the description\n",
    "    # because this didn't need to be translated\n",
    "    description_words_plus = [company_name] + description_words\n",
    "\n",
    "    # variable for containing the description string\n",
    "    description = []\n",
    "\n",
    "    # start looking for description on each page\n",
    "    for page in description_links:\n",
    "        # recall that each element of description links contains both the link and its text\n",
    "        # extract those now\n",
    "        link = page[0]\n",
    "        paragraphs_text = page[1]\n",
    "\n",
    "        # look for key words in paragraphs\n",
    "        for word in description_words_plus:\n",
    "            for paragraph in paragraphs_text:\n",
    "                # remove new lines to simplify and improve results\n",
    "                remove_new_line_pattern = '[\\r\\n]{1,}'\n",
    "                paragraph = re.sub(remove_new_line_pattern, '', paragraph)\n",
    "\n",
    "                # look for the start of a paragraph or a new sentence that contains the key word and at least 2 words\n",
    "                # following. save the whole sentence to description\n",
    "                # an example of a sentence of this form is:\n",
    "                # we are trying to do x OR\n",
    "                # Grofit is selling x\n",
    "                # if I had more time, I might try to look for a verb soon after this key word \"we\" or company name\n",
    "                description = re.findall('[.!?]?[ ]*([^.!?]*'+word+' [^.!? ]+ [^.!?]+[.!?])', paragraph, re.IGNORECASE)\n",
    "                if description:\n",
    "                    # yay, you found a description\n",
    "                    # if  you don't yet have an about page, there's a pretty good chance\n",
    "                    # you're on it now.\n",
    "                    if about_link == \"N/A\":\n",
    "                        about_link = link\n",
    "                    break\n",
    "            if description: break\n",
    "        if description: break\n",
    "\n",
    "    # translate non English descriptions to English\n",
    "    if language != \"en\":\n",
    "        description = translator.translate(description, src=language)\n",
    "\n",
    "    # now try to find contact info: email addresses and phone numbers\n",
    "    for page in contact_info_links:\n",
    "        link = page[0]\n",
    "        paragraphs_text = page[1]\n",
    "\n",
    "        # start with emails\n",
    "        for paragraph in paragraphs_text:\n",
    "            # use a generic email pattern to find emails in paragraphs\n",
    "            new_emails = re.findall(email_pattern, paragraph, re.IGNORECASE)\n",
    "            # if found, add all to emails\n",
    "            if new_emails:\n",
    "                for email in new_emails:\n",
    "                    emails.append(email)\n",
    "\n",
    "            # remove spaces, dashes, and new lines before looking for phone numbers\n",
    "            paragraph = re.sub(remove_spaces_pattern, '', paragraph)\n",
    "\n",
    "            # use a basic phone number pattern to find phone numbers in paragraphs\n",
    "            new_numbers = re.findall(phone_pattern, paragraph)\n",
    "            if new_numbers:\n",
    "                # add them to phone_numbers\n",
    "                for number in new_numbers:\n",
    "                    phone_numbers.append(number)\n",
    "        # if phone numbers have been found on one page, assume that different numbers\n",
    "        # will not be found on another\n",
    "        if phone_numbers: break\n",
    "\n",
    "    # add all values from this company to next row of dataframe\n",
    "    new_df.at[i, \"phone_numbers\"] = phone_numbers\n",
    "    new_df.at[i, \"emails\"] = emails\n",
    "    new_df.at[i, \"contact_page\"] = contact_link\n",
    "    new_df.at[i, \"english_description\"] = description\n",
    "    new_df.at[i, \"about_us_page\"] = about_link\n",
    "    new_df.at[i, \"emails\"] = emails\n",
    "    new_df.at[i, \"privacy_policy\"] = privacy_link\n",
    "    new_df.at[i, \"terms_and_conditions\"] = terms_and_conditions_link\n",
    "\n",
    "    # update the language_words pickle file in case a new language was added\n",
    "    language_words_df.to_pickle('language_words.pkl') \n",
    "    # export final results\n",
    "    new_df.to_excel(r'EutopiaTask1Results.xlsx', index = False, header=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  },
  "kernelspec": {
   "display_name": "Python 3.8.9 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
